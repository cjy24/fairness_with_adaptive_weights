{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "compas.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "etmGLa8X6jep"
      },
      "source": [
        "#load data\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import preprocessing\n",
        "\n",
        "data = pd.read_csv(\".../compas-scores-two-years.csv\")\n",
        "data = pd.DataFrame(data)\n",
        "\n",
        "data_w = data[data['race'] == 'Caucasian']\n",
        "data_b = data[data['race'] == 'African-American']\n",
        "\n",
        "data = pd.concat([data_w, data_b])\n",
        "\n",
        "for col in set(data.columns) - set(data.describe().columns):\n",
        "  data[col] = data[col].astype('category')\n",
        " \n",
        "def clip(df, a):\n",
        "  perm = np.random.permutation(df.index)\n",
        "  m = len(df.index)\n",
        "  clip_end = int(a*m)\n",
        "  cliped = df.iloc[:clip_end]\n",
        "  return cliped\n",
        "\n",
        "def oneHotCatVars(df, df_cols):\n",
        "    \n",
        "    df_1 = df.drop(columns = df_cols, axis = 1)\n",
        "    df_2 = pd.get_dummies(df[df_cols])\n",
        "    \n",
        "    return (pd.concat([df_1, df_2], axis=1, join='inner'))\n",
        "\n",
        "data_preprocessed = oneHotCatVars(data, data.select_dtypes('category').columns)\n",
        "\n",
        " \n",
        "normalize_columns = ['age', 'juv_fel_count', 'juv_misd_count', 'juv_other_count', 'priors_count']\n",
        " \n",
        "def normalize(columns):\n",
        "  scaler = preprocessing.StandardScaler()\n",
        "  data_preprocessed[columns] = scaler.fit_transform(data_preprocessed[columns])\n",
        " \n",
        "normalize(normalize_columns)\n",
        "\n",
        "x_train, x_test = train_test_split(data_preprocessed)\n",
        "\n",
        "b_test = x_test[x_test['race_African-American'] == 1]\n",
        "w_test = x_test[x_test['race_Caucasian'] == 1]\n",
        "\n",
        "train = x_train.drop(['two_year_recid'],axis=1)\n",
        "train = train.drop(['race_African-American'],axis=1)\n",
        "train = train.drop(['race_Caucasian'],axis=1)\n",
        "train_label = x_train['two_year_recid']\n",
        "test_x = x_test.drop(['two_year_recid'],axis=1)\n",
        "test_x = test_x.drop(['race_African-American'],axis=1)\n",
        "test_x = test_x.drop(['race_Caucasian'],axis=1)\n",
        "test_label = x_test['two_year_recid']\n",
        "b_test_x = b_test.drop(['two_year_recid'],axis=1)\n",
        "b_test_x = b_test_x.drop(['race_African-American'],axis=1)\n",
        "b_test_x = b_test_x.drop(['race_Caucasian'],axis=1)\n",
        "b_test_y = b_test['two_year_recid']\n",
        "w_test_x = w_test.drop(['two_year_recid'],axis=1)\n",
        "w_test_x = w_test_x.drop(['race_African-American'],axis=1)\n",
        "w_test_x = w_test_x.drop(['race_Caucasian'],axis=1)\n",
        "w_test_y = w_test['two_year_recid']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFBHRhqrSlNb"
      },
      "source": [
        "#vanilla classifier\n",
        "\n",
        "def model_eval(actual, pred):\n",
        "    \n",
        "    confusion = pd.crosstab(actual, pred, rownames=['Actual'], colnames=['Predicted'])\n",
        "    TP = confusion.loc[1,1]\n",
        "    TN = confusion.loc[0,0]\n",
        "    FP = confusion.loc[0,1]\n",
        "    FN = confusion.loc[1,0]\n",
        "    \n",
        "    out = {}\n",
        "    out['ALL'] = (TP+TN+FP+FN)\n",
        "    out['DP'] = (TP+FP)/(TP+TN+FP+FN)\n",
        "    out['TPR'] =  TP/(TP+FN)\n",
        "    out['TNR'] = TN/(FP+TN)\n",
        "    out['FPR'] = FP/(FP+TN)\n",
        "    out['FNR'] = FN/(TP+FN)\n",
        "    out['ACR'] = (TP+TN)/(TP+TN+FP+FN)\n",
        "    \n",
        "    return out\n",
        "\n",
        "log_reg = LogisticRegression(penalty = 'l2', dual = False, tol = 1e-4, fit_intercept = False, \n",
        "                            max_iter=400, solver='newton-cg', warm_start = True)\n",
        "log_reg.fit(train, train_label)\n",
        "\n",
        "log_reg_pred1 = log_reg.predict(b_test_x)\n",
        "logistic_reg1 = model_eval(b_test_y, log_reg_pred1)\n",
        "ovl_logreg1 = round(pd.DataFrame([logistic_reg1], index = ['logistic_reg_Black']),4)\n",
        "display(ovl_logreg1)\n",
        "\n",
        "log_reg_pred2 = log_reg.predict(w_test_x)\n",
        "logistic_reg2 = model_eval(w_test_y, log_reg_pred2)\n",
        "ovl_logreg2 = round(pd.DataFrame([logistic_reg2], index = ['logistic_reg_White']),4)\n",
        "display(ovl_logreg2)\n",
        "\n",
        "log_reg_pred3 = log_reg.predict(test_x)\n",
        "logistic_reg3 = model_eval(test_label, log_reg_pred3)\n",
        "ovl_logreg3 = round(pd.DataFrame([logistic_reg3], index = ['logistic_reg_All']),4)\n",
        "display(ovl_logreg3)\n",
        "\n",
        "DI = round(100 * abs(logistic_reg2['DP'] - logistic_reg1['DP']), 4)\n",
        "DFPR = round(100 * abs(logistic_reg2['TNR'] - logistic_reg1['TNR']), 4)\n",
        "DFNR = round(100 * abs(logistic_reg2['TPR'] - logistic_reg1['TPR']), 4)\n",
        "prule = round(100 * min(logistic_reg2['DP'] / logistic_reg1['DP'], logistic_reg1['DP'] / logistic_reg2['DP']), 4)\n",
        "ACR = round(logistic_reg3['ACR']*100, 4)\n",
        "\n",
        "print(f'Baseline method: Average accuracy is {ACR}%, Disparate impact is {DI}%, disparate TPR is {DFNR}%,\\n disparate TNR is {DFPR}%, p% rule is {prule}%.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szA42CNBoL28"
      },
      "source": [
        "from sklearn.metrics import log_loss\n",
        "import math\n",
        "import cvxpy as cp\n",
        "\n",
        "wi0 = np.ones((x_train.shape[0]))\n",
        "wi1 = wi0 + 10000000000000\n",
        "\n",
        "\n",
        "x_train_b = x_train[x_train['race_African-American'] == 1]\n",
        "x_train_bp = x_train_b[x_train_b['two_year_recid'] == 1]\n",
        "x_train_bn = x_train_b[x_train_b['two_year_recid'] == 0]\n",
        "x_train_w = x_train[x_train['race_Caucasian'] == 1]\n",
        "x_train_wp = x_train_w[x_train_w['two_year_recid'] == 1]\n",
        "x_train_wn = x_train_w[x_train_w['two_year_recid'] == 0]\n",
        "train_bp = x_train_bp.drop(['two_year_recid'],axis=1)\n",
        "train_bp = train_bp.drop(['race_African-American'],axis=1)\n",
        "train_bp = train_bp.drop(['race_Caucasian'],axis=1)\n",
        "bp_label = x_train_bp['two_year_recid'].to_numpy()\n",
        "train_bn = x_train_bn.drop(['two_year_recid'],axis=1)\n",
        "train_bn = train_bn.drop(['race_African-American'],axis=1)\n",
        "train_bn = train_bn.drop(['race_Caucasian'],axis=1)\n",
        "bn_label = x_train_bn['two_year_recid'].to_numpy()\n",
        "train_wp = x_train_wp.drop(['two_year_recid'],axis=1)\n",
        "train_wp = train_wp.drop(['race_African-American'],axis=1)\n",
        "train_wp = train_wp.drop(['race_Caucasian'],axis=1)\n",
        "wp_label = x_train_wp['two_year_recid'].to_numpy()\n",
        "train_wn = x_train_wn.drop(['two_year_recid'],axis=1)\n",
        "train_wn = train_wn.drop(['race_African-American'],axis=1)\n",
        "train_wn = train_wn.drop(['race_Caucasian'],axis=1)\n",
        "wn_label = x_train_wn['two_year_recid'].to_numpy()\n",
        "\n",
        "loss_bp = wi0_bp = np.ones(x_train_bp.shape[0])\n",
        "loss_bn = wi0_bn = np.ones(x_train_bn.shape[0])\n",
        "loss_wp = wi0_wp = np.ones(x_train_wp.shape[0])\n",
        "loss_wn = wi0_wn = np.ones(x_train_wn.shape[0])\n",
        "\n",
        "def optim(loss, a, c):\n",
        "  A = loss\n",
        "  x = cp.Variable(loss.shape[0])\n",
        "  objective = cp.Maximize(-a*cp.sum_squares(x)+cp.sum(cp.multiply(A,x)))\n",
        "  constraints = [0 <= x, cp.sum(x) == c]\n",
        "  prob = cp.Problem(objective, constraints)   \n",
        "  result = prob.solve()\n",
        "  for i in range(x.value.shape[0]):\n",
        "    if abs(x.value[i]) < 0.01 or x.value[i] < 0:\n",
        "      x.value[i] = 0\n",
        "  x.value = x.value\n",
        "  return x.value\n",
        "\n",
        "def model_eval(actual, pred):\n",
        "    \n",
        "    confusion = pd.crosstab(actual, pred, rownames=['Actual'], colnames=['Predicted'])\n",
        "    TP = confusion.loc[1,1]\n",
        "    TN = confusion.loc[0,0]\n",
        "    FP = confusion.loc[0,1]\n",
        "    FN = confusion.loc[1,0]\n",
        "    \n",
        "    out = {}\n",
        "    out['ALL'] = (TP+TN+FP+FN)\n",
        "    out['DP'] = (TP+FP)/(TP+TN+FP+FN)\n",
        "    out['TPR'] =  TP/(TP+FN)\n",
        "    out['TNR'] = TN/(FP+TN)\n",
        "    out['FPR'] = FP/(FP+TN)\n",
        "    out['FNR'] = FN/(TP+FN)\n",
        "    out['ACR'] = (TP+TN)/(TP+TN+FP+FN)\n",
        "    \n",
        "    return out\n",
        "\n",
        "def dif(a, b):\n",
        "  sum = 0\n",
        "  for i in range(len(a)):\n",
        "    sum += (a[i] - b[i]) ** 2\n",
        "  sum0 = sum ** 0.5\n",
        "  return sum0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aITr5ltFkUF"
      },
      "source": [
        "def Random_search(train, folds, param_range, num_param = 10):\n",
        "    trainscore = []\n",
        "    testscore = []\n",
        "    K_values = sample(range(param_range[0],param_range[1]),num_param)\n",
        "    K_values.sort()\n",
        "    print('k range:',K_values)\n",
        "    for k in tqdm(K_values):\n",
        "        trainscore_folds = []\n",
        "        testscore_folds = []\n",
        "        for j in range(0, folds):\n",
        "            x_train, x_test = train_test_split(train)\n",
        "            Y_train = x_train['two_year_recid'].to_numpy()\n",
        "            X_train = x_train.drop(['two_year_recid'],axis=1)\n",
        "            X_train = X_train.drop(['race_African-American'],axis=1)\n",
        "            X_train = X_train.drop(['race_Caucasian'],axis=1).to_numpy()\n",
        "            Y_test = x_test['two_year_recid'].to_numpy()\n",
        "            X_test = x_test.drop(['two_year_recid'],axis=1)\n",
        "            X_test = X_test.drop(['race_African-American'],axis=1)\n",
        "            X_test = X_test.drop(['race_Caucasian'],axis=1).to_numpy()\n",
        "\n",
        "            iter = 0\n",
        "            wi0 = np.ones((X_train.shape[0]))\n",
        "            wi1 = wi0 + 10000000000000\n",
        "            x_train_b = x_train[x_train['race_African-American'] == 1]\n",
        "            x_train_bp = x_train_b[x_train_b['two_year_recid'] == 1]\n",
        "            x_train_bn = x_train_b[x_train_b['two_year_recid'] == 0]\n",
        "            x_train_w = x_train[x_train['race_Caucasian'] == 1]\n",
        "            x_train_wp = x_train_w[x_train_w['two_year_recid'] == 1]\n",
        "            x_train_wn = x_train_w[x_train_w['two_year_recid'] == 0]\n",
        "            train_bp = x_train_bp.drop(['two_year_recid'],axis=1)\n",
        "            train_bp = train_bp.drop(['race_African-American'],axis=1)\n",
        "            train_bp = train_bp.drop(['race_Caucasian'],axis=1).to_numpy()\n",
        "            bp_label = x_train_bp['two_year_recid'].to_numpy()\n",
        "            train_bn = x_train_bn.drop(['two_year_recid'],axis=1)\n",
        "            train_bn = train_bn.drop(['race_African-American'],axis=1)\n",
        "            train_bn = train_bn.drop(['race_Caucasian'],axis=1).to_numpy()\n",
        "            bn_label = x_train_bn['two_year_recid'].to_numpy()\n",
        "            train_wp = x_train_wp.drop(['two_year_recid'],axis=1)\n",
        "            train_wp = train_wp.drop(['race_African-American'],axis=1)\n",
        "            train_wp = train_wp.drop(['race_Caucasian'],axis=1).to_numpy()\n",
        "            wp_label = x_train_wp['two_year_recid'].to_numpy()\n",
        "            train_wn = x_train_wn.drop(['two_year_recid'],axis=1)\n",
        "            train_wn = train_wn.drop(['race_African-American'],axis=1)\n",
        "            train_wn = train_wn.drop(['race_Caucasian'],axis=1).to_numpy()\n",
        "            wn_label = x_train_wn['two_year_recid'].to_numpy()\n",
        "\n",
        "            loss_bp = wi0_bp = np.ones(x_train_bp.shape[0])\n",
        "            loss_bn = wi0_bn = np.ones(x_train_bn.shape[0])\n",
        "            loss_wp = wi0_wp = np.ones(x_train_wp.shape[0])\n",
        "            loss_wn = wi0_wn = np.ones(x_train_wn.shape[0])\n",
        "\n",
        "            while dif(wi0, wi1) > 0.0001:\n",
        "              classifier = LogisticRegression(penalty = 'none', dual = False, tol = 1e-4, fit_intercept = False, \n",
        "                            max_iter=400, solver='newton-cg', warm_start = True)\n",
        "              classifier.fit(X_train, Y_train, wi0)\n",
        "\n",
        "              for i1 in range(train_bp.shape[0]):\n",
        "                loss_bp[i1] = -math.log(log_reg.predict_proba(train_bp)[i1][1]+1e-100)\n",
        "\n",
        "              for i2 in range(train_bn.shape[0]):\n",
        "                loss_bn[i2] = -math.log(log_reg.predict_proba(train_bn)[i2][0]+1e-100)\n",
        "\n",
        "              for i3 in range(train_wp.shape[0]):\n",
        "                loss_wp[i3] = -math.log(log_reg.predict_proba(train_wp)[i3][1]+1e-100)\n",
        "\n",
        "              for i4 in range(train_wn.shape[0]):\n",
        "                loss_wn[i4] = -math.log(log_reg.predict_proba(train_wn)[i4][0]+1e-100)\n",
        "\n",
        "              wi1 = wi0\n",
        "              wi0_bp_1 = optim(loss_bp, k, c)\n",
        "              wi0_bn_1 = optim(loss_bn, k, c)\n",
        "              wi0_wp_1 = optim(loss_wp, k, c)\n",
        "              wi0_wn_1 = optim(loss_wn, k, c)\n",
        "\n",
        "              X_train = np.concatenate([train_bp, train_bn, train_wp, train_wn])\n",
        "              Y_train = np.concatenate((bp_label, bn_label, wp_label, wn_label))\n",
        "              wi0 = np.concatenate((wi0_bp_1, wi0_bn_1, wi0_wp_1, wi0_wn_1))\n",
        "\n",
        "              iter = iter + 1\n",
        "\n",
        "\n",
        "            Y_predicted = classifier.predict(X_test)\n",
        "            testscore_folds.append(np.sum(Y_test == Y_predicted)/len(Y_test))\n",
        "\n",
        "            Y_predicted = classifier.predict(X_train)\n",
        "            trainscore_folds.append(np.sum(Y_train == Y_predicted)/len(Y_train))\n",
        "        trainscore.append(np.mean(np.array(trainscore_folds)))\n",
        "        testscore.append(np.mean(np.array(testscore_folds)))\n",
        "    return trainscore, testscore , K_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.ones(10)\n",
        "b = np.ones(10)\n",
        "np.sum(a == b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRBz2CvjLbIY",
        "outputId": "ff0e8df2-5114-4407-9666-6fa747211c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from random import sample \n",
        "\n",
        "param_min = 1 \n",
        "param_max = 20 \n",
        "k_fold = 5\n",
        "param_range = (param_min, param_max) \n",
        "trainscore, testscore, K_values = Random_search(x_train, k_fold, param_range)"
      ],
      "metadata": {
        "id": "hK0Xh-MLAaJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdoyG4ukUCuw"
      },
      "source": [
        "iter = 0\n",
        "\n",
        "while dif(wi0, wi1) > 0.0001:\n",
        "  log_reg = LogisticRegression(penalty = 'none', dual = False, tol = 1e-4, fit_intercept = False, \n",
        "                            max_iter=400, solver='newton-cg', warm_start = True)\n",
        "  log_reg.fit(train, train_label, wi0)\n",
        "\n",
        "  for i1 in range(train_bp.shape[0]):\n",
        "    loss_bp[i1] = -math.log(log_reg.predict_proba(train_bp)[i1][1]+1e-100)\n",
        "\n",
        "  for i2 in range(train_bn.shape[0]):\n",
        "    loss_bn[i2] = -math.log(log_reg.predict_proba(train_bn)[i2][0]+1e-100)\n",
        "\n",
        "  for i3 in range(train_wp.shape[0]):\n",
        "    loss_wp[i3] = -math.log(log_reg.predict_proba(train_wp)[i3][1]+1e-100)\n",
        "\n",
        "  for i4 in range(train_wn.shape[0]):\n",
        "    loss_wn[i4] = -math.log(log_reg.predict_proba(train_wn)[i4][0]+1e-100)\n",
        "\n",
        "  wi1 = wi0\n",
        "  wi0_bp_1 = optim(loss_bp, a, c)\n",
        "  wi0_bn_1 = optim(loss_bn, a, c)\n",
        "  wi0_wp_1 = optim(loss_wp, a, c)\n",
        "  wi0_wn_1 = optim(loss_wn, a, c)\n",
        "\n",
        "  train = pd.concat([train_bp, train_bn, train_wp, train_wn])\n",
        "  train_label = np.concatenate((bp_label, bn_label, wp_label, wn_label))\n",
        "  wi0 = np.concatenate((wi0_bp_1, wi0_bn_1, wi0_wp_1, wi0_wn_1))\n",
        "\n",
        "  iter = iter + 1\n",
        "\n",
        "\n",
        "  log_reg_pred1 = log_reg.predict(b_test_x)\n",
        "  logistic_reg1 = model_eval(b_test_y, log_reg_pred1)\n",
        "  ovl_logreg1 = round(pd.DataFrame([logistic_reg1], index = ['logistic_reg_Black']),4)\n",
        "  display(ovl_logreg1)\n",
        "\n",
        "  log_reg_pred2 = log_reg.predict(w_test_x)\n",
        "  logistic_reg2 = model_eval(w_test_y, log_reg_pred2)\n",
        "  ovl_logreg2 = round(pd.DataFrame([logistic_reg2], index = ['logistic_reg_White']),4)\n",
        "  display(ovl_logreg2)\n",
        "\n",
        "  log_reg_pred3 = log_reg.predict(test_x)\n",
        "  logistic_reg3 = model_eval(test_label, log_reg_pred3)\n",
        "  ovl_logreg3 = round(pd.DataFrame([logistic_reg3], index = ['logistic_reg_All']),4)\n",
        "  display(ovl_logreg3)\n",
        "\n",
        "  DI = 100 * abs(logistic_reg2['DP'] - logistic_reg1['DP'])\n",
        "  DFPR = 100 * abs(logistic_reg2['TNR'] - logistic_reg1['TNR'])\n",
        "  DFNR = 100 * abs(logistic_reg2['TPR'] - logistic_reg1['TPR'])\n",
        "  prule = 100 * min(logistic_reg2['DP'] / logistic_reg1['DP'], logistic_reg1['DP'] / logistic_reg2['DP'])\n",
        "\n",
        "  print(f'Our method: Iteration {iter-1}, disparate impact is {DI}%, disparate FPR is {DFPR}%,\\n disparate FNR is {DFNR}%, p% rule is {prule}%ï¼Œ\\n disparate accuracy is {DACR}%.')\n",
        "\n",
        "log_reg_pred1 = log_reg.predict(b_test_x)\n",
        "logistic_reg1 = model_eval(b_test_y, log_reg_pred1)\n",
        "ovl_logreg1 = round(pd.DataFrame([logistic_reg1], index = ['logistic_reg_Black']),4)\n",
        "display(ovl_logreg1)\n",
        "\n",
        "log_reg_pred2 = log_reg.predict(w_test_x)\n",
        "logistic_reg2 = model_eval(w_test_y, log_reg_pred2)\n",
        "ovl_logreg2 = round(pd.DataFrame([logistic_reg2], index = ['logistic_reg_White']),4)\n",
        "display(ovl_logreg2)\n",
        "\n",
        "log_reg_pred3 = log_reg.predict(test_x)\n",
        "logistic_reg3 = model_eval(test_label, log_reg_pred3)\n",
        "ovl_logreg3 = round(pd.DataFrame([logistic_reg3], index = ['logistic_reg_All']),4)\n",
        "display(ovl_logreg3)\n",
        "\n",
        "DI = 100 * abs(logistic_reg2['DP'] - logistic_reg1['DP'])\n",
        "DFPR = 100 * abs(logistic_reg2['TNR'] - logistic_reg1['TNR'])\n",
        "DFNR = 100 * abs(logistic_reg2['TPR'] - logistic_reg1['TPR'])\n",
        "prule = 100 * min(logistic_reg2['DP'] / logistic_reg1['DP'], logistic_reg1['DP'] / logistic_reg2['DP'])\n",
        "\n",
        "print(f'Our method: Disparate impact is {DI}%, disparate FPR is {DFPR}%,\\n disparate FNR is {DFNR}%, p% rule is {prule}%.')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}